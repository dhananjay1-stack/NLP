{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZXTtUqwvY9h",
        "outputId": "6472af19-38de-4e25-f2c9-0a40ea2c7355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import (WhitespaceTokenizer,\n",
        "                           WordPunctTokenizer,\n",
        "                           TreebankWordTokenizer,\n",
        "                           TweetTokenizer,\n",
        "                           MWETokenizer)\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "# --- SAMPLE TEXTS ---\n",
        "# Text 1: Standard text for general tokenization\n",
        "text_std = \"The quick brown fox jumps over the lazy dog. It's awesome!\"\n",
        "\n",
        "# Text 2: Text with Multi-Word Expressions (MWE) like 'New York'\n",
        "text_mwe = \"I love New York and machine learning is fun.\"\n",
        "\n",
        "# Text 3: Tweet text with emojis and hashtags\n",
        "text_tweet = \"Just finished the NLP assignment! :D #NLP #Python @GoogleColab\""
      ],
      "metadata": {
        "id": "b6Ohk_zQvd6C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. TOKENIZATION\n",
        "# ==========================================\n",
        "print(\"--- 1. TOKENIZATION ---\")\n",
        "\n",
        "# A. Whitespace Tokenization\n",
        "ws_tokenizer = WhitespaceTokenizer()\n",
        "print(f\"Whitespace: {ws_tokenizer.tokenize(text_std)}\")\n",
        "\n",
        "# B. Punctuation-based Tokenization\n",
        "# Splits text on whitespace and punctuation\n",
        "wp_tokenizer = WordPunctTokenizer()\n",
        "print(f\"Punctuation-based: {wp_tokenizer.tokenize(text_std)}\")\n",
        "\n",
        "# C. Treebank Tokenization (Standard NLTK method)\n",
        "# Uses standard English grammar conventions (separates \"It's\" into \"It\" and \"'s\")\n",
        "tb_tokenizer = TreebankWordTokenizer()\n",
        "print(f\"Treebank: {tb_tokenizer.tokenize(text_std)}\")\n",
        "\n",
        "# D. Tweet Tokenization\n",
        "# Preserves emojis and hashtags usually lost in other tokenizers\n",
        "tw_tokenizer = TweetTokenizer()\n",
        "print(f\"Tweet: {tw_tokenizer.tokenize(text_tweet)}\")\n",
        "\n",
        "# E. MWE (Multi-Word Expression) Tokenization\n",
        "# You must manually define which words belong together\n",
        "mwe_tokenizer = MWETokenizer()\n",
        "mwe_tokenizer.add_mwe(('New', 'York'))        # Add 'New York' as a single token\n",
        "mwe_tokenizer.add_mwe(('machine', 'learning')) # Add 'machine learning'\n",
        "# Note: MWETokenizer requires a list of strings as input, not raw text\n",
        "# We use standard split() first to feed it tokens\n",
        "print(f\"MWE: {mwe_tokenizer.tokenize(text_mwe.split())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f2i8rAqvo7P",
        "outputId": "2ba9707c-c661-4d2c-b008-2eb18191a4e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. TOKENIZATION ---\n",
            "Whitespace: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.', \"It's\", 'awesome!']\n",
            "Punctuation-based: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'\", 's', 'awesome', '!']\n",
            "Treebank: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.', 'It', \"'s\", 'awesome', '!']\n",
            "Tweet: ['Just', 'finished', 'the', 'NLP', 'assignment', '!', ':D', '#NLP', '#Python', '@GoogleColab']\n",
            "MWE: ['I', 'love', 'New_York', 'and', 'machine_learning', 'is', 'fun.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. STEMMING\n",
        "# ==========================================\n",
        "print(\"\\n--- 2. STEMMING ---\")\n",
        "words_to_stem = [\"running\", \"generous\", \"happily\", \"organization\", \"wolves\"]\n",
        "\n",
        "# A. Porter Stemmer (Older, more aggressive)\n",
        "porter = PorterStemmer()\n",
        "porter_results = [porter.stem(w) for w in words_to_stem]\n",
        "print(f\"Original Words: {words_to_stem}\")\n",
        "print(f\"Porter Stemmer: {porter_results}\")\n",
        "\n",
        "# B. Snowball Stemmer (Newer, slightly more accurate)\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "snowball_results = [snowball.stem(w) for w in words_to_stem]\n",
        "print(f\"Snowball Stemmer: {snowball_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjP28PAdv-t8",
        "outputId": "816b14bd-9ffc-4483-d3cb-67864e01cfbc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2. STEMMING ---\n",
            "Original Words: ['running', 'generous', 'happily', 'organization', 'wolves']\n",
            "Porter Stemmer: ['run', 'gener', 'happili', 'organ', 'wolv']\n",
            "Snowball Stemmer: ['run', 'generous', 'happili', 'organ', 'wolv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. LEMMATIZATION\n",
        "# ==========================================\n",
        "print(\"\\n--- 3. LEMMATIZATION ---\")\n",
        "# Lemmatization reduces words to their base dictionary form (Lemma)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words_to_lemmatize = [\"running\", \"corpora\", \"better\", \"rocks\"]\n",
        "\n",
        "# Note: Lemmatizers work best when provided context (POS tags).\n",
        "# Without context, it assumes everything is a Noun.\n",
        "lemma_results = [lemmatizer.lemmatize(w) for w in words_to_lemmatize]\n",
        "\n",
        "# Example with explicit POS tagging (v=verb, a=adjective)\n",
        "lemma_verb = lemmatizer.lemmatize(\"running\", pos=\"v\")     # Should become 'run'\n",
        "lemma_adj = lemmatizer.lemmatize(\"better\", pos=\"a\")       # Should become 'good'\n",
        "\n",
        "print(f\"Basic Lemmatization: {lemma_results}\")\n",
        "print(f\"'running' as verb: {lemma_verb}\")\n",
        "print(f\"'better' as adjective: {lemma_adj}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jya8mxA5wBSa",
        "outputId": "d00cd515-c058-4849-d439-575a8e1a7eb5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 3. LEMMATIZATION ---\n",
            "Basic Lemmatization: ['running', 'corpus', 'better', 'rock']\n",
            "'running' as verb: run\n",
            "'better' as adjective: good\n"
          ]
        }
      ]
    }
  ]
}