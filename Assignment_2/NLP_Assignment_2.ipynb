{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPmVq04Iyk45",
        "outputId": "32a641a5-0ac7-4ad0-dfaf-f46bf7cee494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install gensim\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download tokenizer for Word2Vec preprocessing\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"I love machine learning and deep learning\",\n",
        "    \"I love coding in python\",\n",
        "    \"Machine learning is fun and exciting\"\n",
        "]\n",
        "\n",
        "print(\"--- INPUT CORPUS ---\")\n",
        "for i, doc in enumerate(corpus):\n",
        "    print(f\"Doc {i+1}: {doc}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtXB8zwry5PT",
        "outputId": "1a4dfcdc-12bd-4222-ef1b-81a2a1afdbb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- INPUT CORPUS ---\n",
            "Doc 1: I love machine learning and deep learning\n",
            "Doc 2: I love coding in python\n",
            "Doc 3: Machine learning is fun and exciting\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 1. BAG OF WORDS (BoW) ---\")\n",
        "\n",
        "# A. Count Occurrence (Raw Counts)\n",
        "# CountVectorizer converts text to a matrix of token counts\n",
        "count_vect = CountVectorizer()\n",
        "bow_matrix = count_vect.fit_transform(corpus)\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=count_vect.get_feature_names_out())\n",
        "print(\"A. Raw Count Occurrence Matrix:\")\n",
        "print(bow_df)\n",
        "print(\"\\n\")\n",
        "\n",
        "# B. Normalized Count Occurrence (Term Frequency)\n",
        "# Normalization (L1) ensures that the sum of the row is 1 (representing probabilities/frequencies)\n",
        "# This accounts for document length differences.\n",
        "norm_count_vect = CountVectorizer()\n",
        "bow_matrix_raw = norm_count_vect.fit_transform(corpus).toarray()\n",
        "\n",
        "# Manually normalize: divide each count by the total words in that document (row sum)\n",
        "# axis=1 means sum across columns (per row)\n",
        "# [:, np.newaxis] allows division of the matrix by the column vector\n",
        "normalized_matrix = bow_matrix_raw / bow_matrix_raw.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "norm_bow_df = pd.DataFrame(normalized_matrix, columns=norm_count_vect.get_feature_names_out())\n",
        "print(\"B. Normalized Count Matrix (Term Frequency):\")\n",
        "print(norm_bow_df.round(2)) # Rounded for readability\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbjubPLgy7E_",
        "outputId": "84428445-4ce3-4685-b2e6-41c5d023737d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. BAG OF WORDS (BoW) ---\n",
            "A. Raw Count Occurrence Matrix:\n",
            "   and  coding  deep  exciting  fun  in  is  learning  love  machine  python\n",
            "0    1       0     1         0    0   0   0         2     1        1       0\n",
            "1    0       1     0         0    0   1   0         0     1        0       1\n",
            "2    1       0     0         1    1   0   1         1     0        1       0\n",
            "\n",
            "\n",
            "B. Normalized Count Matrix (Term Frequency):\n",
            "    and  coding  deep  exciting   fun    in    is  learning  love  machine  \\\n",
            "0  0.17    0.00  0.17      0.00  0.00  0.00  0.00      0.33  0.17     0.17   \n",
            "1  0.00    0.25  0.00      0.00  0.00  0.25  0.00      0.00  0.25     0.00   \n",
            "2  0.17    0.00  0.00      0.17  0.17  0.00  0.17      0.17  0.00     0.17   \n",
            "\n",
            "   python  \n",
            "0    0.00  \n",
            "1    0.25  \n",
            "2    0.00  \n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 2. TF-IDF ---\")\n",
        "\n",
        "# TfidfVectorizer converts text to a matrix of TF-IDF features\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vect.fit_transform(corpus)\n",
        "\n",
        "# Convert to DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vect.get_feature_names_out())\n",
        "\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_df.round(2))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYVRQIoKzDiy",
        "outputId": "b6d0abd7-2a1b-4478-fcec-dc31ab0725c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 2. TF-IDF ---\n",
            "TF-IDF Matrix:\n",
            "    and  coding  deep  exciting   fun    in    is  learning  love  machine  \\\n",
            "0  0.34    0.00  0.45      0.00  0.00  0.00  0.00      0.68  0.34     0.34   \n",
            "1  0.00    0.53  0.00      0.00  0.00  0.53  0.00      0.00  0.40     0.00   \n",
            "2  0.35    0.00  0.00      0.46  0.46  0.00  0.46      0.35  0.00     0.35   \n",
            "\n",
            "   python  \n",
            "0    0.00  \n",
            "1    0.53  \n",
            "2    0.00  \n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "print(\"--- 3. WORD2VEC ---\")\n",
        "\n",
        "# Word2Vec requires a list of tokenized sentences (list of lists of words)\n",
        "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
        "print(f\"Tokenized Input for Word2Vec: {tokenized_corpus}\\n\")\n",
        "\n",
        "# Train the Word2Vec model\n",
        "# vector_size=10: Creates a vector of 10 dimensions for each word (usually 100-300 in real use)\n",
        "# window=5: Context window size\n",
        "# min_count=1: Ignores words that appear less than 1 time\n",
        "# workers=4: Number of threads\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=10, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Access vector for a specific word (e.g., 'learning')\n",
        "word_to_check = 'learning'\n",
        "vector = model.wv[word_to_check]\n",
        "\n",
        "print(f\"Vector for word '{word_to_check}' (Size 10):\")\n",
        "print(vector)\n",
        "\n",
        "# Find most similar words (based on cosine similarity)\n",
        "# Note: With such a tiny dataset, similarity results won't be very meaningful yet\n",
        "print(f\"\\nMost similar words to '{word_to_check}':\")\n",
        "print(model.wv.most_similar(word_to_check))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQbhD8ADzUkd",
        "outputId": "29b3ec4f-f42f-46f6-892d-8ee0a5358384"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 3. WORD2VEC ---\n",
            "Tokenized Input for Word2Vec: [['i', 'love', 'machine', 'learning', 'and', 'deep', 'learning'], ['i', 'love', 'coding', 'in', 'python'], ['machine', 'learning', 'is', 'fun', 'and', 'exciting']]\n",
            "\n",
            "Vector for word 'learning' (Size 10):\n",
            "[-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809\n",
            "  0.06458873  0.08972988 -0.05015428 -0.03763372]\n",
            "\n",
            "Most similar words to 'learning':\n",
            "[('and', 0.5436005592346191), ('coding', 0.43182477355003357), ('python', 0.37924280762672424), ('machine', 0.3004249036312103), ('deep', 0.22743143141269684), ('love', 0.10494352877140045), ('is', -0.1311161071062088), ('fun', -0.18982969224452972), ('i', -0.22418655455112457), ('exciting', -0.2726021111011505)]\n"
          ]
        }
      ]
    }
  ]
}